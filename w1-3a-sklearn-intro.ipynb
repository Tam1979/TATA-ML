{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Introduction\n",
    "\n",
    "Scikit-learn is a library for machine learning in Python.  It helps us in all of the following ways:\n",
    "\n",
    "    1. Preprocessing: Getting the data into shape for Machine Learning\n",
    "    2. Dimensionality Reduction: Reducing redundancy in variables\n",
    "    3. Classification: Predicting one of a finite set of classes for data.\n",
    "    4. Regression: Predicting a response variable\n",
    "    5. Clustering: Finding natural patterns in the data.\n",
    "    6. Model Selection: Finding the best model for our data.\n",
    "\n",
    "\n",
    "We will be looking at our NYCFlights13 dataset here.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cross_validation  import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "flights = pd.read_csv('https://s3.amazonaws.com/elephantscale-public/data/nycflights13/flights.csv.gz')\n",
    "weather = pd.read_csv('https://s3.amazonaws.com/elephantscale-public/data/nycflights13/weather.csv.gz')\n",
    "airports = pd.read_csv('https://s3.amazonaws.com/elephantscale-public/data/nycflights13/airports.csv.gz')\n",
    "\n",
    "df_withweather = pd.merge(flights, weather, how='left', on=['year','month', 'day', 'hour', 'origin'])\n",
    "df = pd.merge(df_withweather, airports, how='left', left_on='dest', right_on='faa')\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the data\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "\n",
    "Let's create a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = 'dep_delay'\n",
    "features =  ['month','day','dep_time','arr_time','carrier','dest','air_time','distance', \n",
    "             'lat', 'lon', 'alt',  'dewp', 'humid', 'wind_speed', 'wind_gust', \n",
    "             'precip', 'pressure', 'visib' ]\n",
    "\n",
    "features_v = df[features]\n",
    "pred_v = df[pred]\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# carrier is not a number, so transform it into an number\n",
    "features_v['carrier'] = pd.factorize(features_v['carrier'])[0]\n",
    "\n",
    "# dest is not a number, so transform it into a number\n",
    "features_v['dest'] = pd.factorize(features_v['dest'])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's look at our feature vector\n",
    "\n",
    "features_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the feature vector\n",
    "\n",
    "Let's scale the feature vector.\n",
    "\n",
    "First the standard scaler substracts by the mean and divides by std.  Let's try that ourselves and see if that looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaled_v = (features_v - features_v.mean()) / features_v.std()\n",
    "scaled_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply that transformation with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how the magnitude of the dimensions is wildly different. Let's try scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_v)\n",
    "scaled_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Dimensions\n",
    "\n",
    "Let's use PCA to reduce dimensions down to two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(scaled_features).transform(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the components of the two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The components are the two axes of the projected back into the original features space.\n",
    "# We can use these to calculate the relative components of the original features in our \n",
    "# new features.\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the top components of PC1:\n",
    "print(\"top components of PC1:\")\n",
    "\n",
    "\n",
    "rel_values = np.abs(pca.components_[0])/np.sum(np.abs(pca.components_[0]))\n",
    "print(\"Feature Names: \" + str([features[i] for i in np.argsort(-rel_values)[:3]]))\n",
    "print(\"Percentages: \" + str(rel_values[np.argsort(-rel_values)[:3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly about 75% of the variance of PC1 is explained by distance, air time, and longtitude.  These variables roughtly correspond to how long the flight is (as longitude will increase as we fly west from NYC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's get the top components of PC2:\n",
    "print(\"top components of PC2:\")\n",
    "\n",
    "rel_values_pc2 = np.abs(pca.components_[1])/np.sum(np.abs(pca.components_[1]))\n",
    "print(\"Feature Names: \" + str([features[i] for i in np.argsort(-rel_values_pc2)[:6]]))\n",
    "print(\"Percentages: \" + str(rel_values_pc2[np.argsort(-rel_values_pc2)[:6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have about half the variance is explained by the weather variables humidity, wind_speed, wind_guest, and dewpoint.  dep_time and arr_time together give us about 25% more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the explained variance of the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means only about a third  of the data is explained by the two dimensions. Reducing dimensions was useful for plotting but not good as a way of capturing most of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "Let's do a quick plot of the data. Because we have many dimensions and we want a 2-D plot, we need to reduce dimensions down to 2.  We can do this with PCA, which will reduce the dimensions to only two by combining redundant features into two principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "\n",
    "plt.xlabel(\"PC1: (Dist, Air Time, Longitude)\")\n",
    "plt.ylabel(\"PC2: (weather vars)\")\n",
    "\n",
    "\n",
    "plt.scatter(X_r[:,0], X_r[:,1], alpha=.8, lw=lw)\n",
    "plt.title('PCA of flights dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How to interpret these clusters? \n",
    "\n",
    "The X axis (PC1), remember, largely is based on the length of the flight.  It ranges in certain bounds, because these are domestic flights. One can only fly so far in a domestic flight.  The outlier section on the far right likely consists of Alaska, Hawaii, and other flights that are abnormally long for  being domestic flights.\n",
    "\n",
    "The Y Axis (PC2), is largely based on weather related variables. Note the large number of small \"outlier\" clusters on the top. These are likely a result of abnormal weather conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplot\n",
    "\n",
    "A biplot will plot both the points and the original feature vectors on the same axis, so you can visualize roughly how the original features are projected to the dimensionality \n",
    "reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def biplot(score,coeff,y,labels=None):\n",
    "    plt.rcParams['figure.figsize'] = [15, 10]\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "    plt.xlim(-0.8,0.7)\n",
    "    plt.ylim(-0.6,0.5)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()\n",
    "    \n",
    "# Let's do a biplot of a PCA = 2 dimensions\n",
    "biplot(X_r[:,0:2],np.transpose(pca.components_[0:2, :]),pred_v,labels=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
